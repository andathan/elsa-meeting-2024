
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":["anastasia"],"categories":null,"content":"Short bio Alumna of ENS Ulm, holding a Ph.D. in Philosophy, Anastasia Stasenko is Research Lead at a French startup opsci.ai. She specialises in designing and operationalising trustworthy and culturally-attuned AI models for public and private sectors. Among her recent projects is the development of Albert, the first gen AI model assisting French public servants in responding to citizens’ enquiries, in collaboration with étalab. In addition to her role at opsci.ai, she holds the position of Associate Senior Lecturer at Sorbonne-Nouvelle. There, she co-leads the Master’s program in Digital Communications and Data Analysis and instructs on Mixed-Methods Data Analysis, Social Media Intelligence, and Knowledge Management Strategies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d5009743fab571c6842bc738d5560ec1","permalink":"https://lix.polytechnique.fr/ethicalai/author/anastasia-stasenko/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/anastasia-stasenko/","section":"authors","summary":"Short bio Alumna of ENS Ulm, holding a Ph.D. in Philosophy, Anastasia Stasenko is Research Lead at a French startup opsci.ai. She specialises in designing and operationalising trustworthy and culturally-attuned AI models for public and private sectors.","tags":null,"title":"Anastasia Stasenko","type":"authors"},{"authors":["bilel"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fd026c6ca39ee77b1ecac2768eac05d5","permalink":"https://lix.polytechnique.fr/ethicalai/author/bilel-benbouzid/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/bilel-benbouzid/","section":"authors","summary":"","tags":null,"title":"Bilel Benbouzid","type":"authors"},{"authors":["catuscia"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"76ca84917217699d946e1135a77f2251","permalink":"https://lix.polytechnique.fr/ethicalai/author/catuscia-palamidessi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/catuscia-palamidessi/","section":"authors","summary":"","tags":null,"title":"Catuscia Palamidessi","type":"authors"},{"authors":["fioretto"],"categories":null,"content":"Title: Privacy and Fairness in Societal Systems Abstract Differential Privacy has become the go-to approach for protecting sensitive information in data releases and learning tasks that are used for critical decision processes. For example, census data is used to allocate funds and distribute benefits, while several corporations use machine learning systems for criminal assessments, hiring decisions, and more. While this privacy notion provides strong guarantees, we will show that it may also induce biases and fairness issues in downstream decision processes. These issues may adversely affect many individuals’ health, well-being, and sense of belonging, and are currently poorly understood.\nIn this talk, we delve into the intersection of privacy, fairness, and decision processes, with a focus on understanding and addressing these fairness issues. We first provide an overview of Differential Privacy and its applications in data release and learning tasks. Next, we examine the societal impacts of privacy through a fairness lens and present a framework to illustrate what aspects of the private algorithms and/or data may be responsible for exacerbating unfairness. We hence show how to extend this framework to assess the disparate impacts arising in Machine Learning tasks. Finally, we propose a path to partially mitigate these fairness issues and discuss grand challenges that require further exploration.\nShort bio Ferdinando Fioretto is an assistant professor at University of Virginia. He works at the juncture of Machine Learning, optimization, privacy, and ethics. His recent work focuses on two themes: (1) it analyzes the equity of AI systems in support of decision-making and learning tasks and designs algorithms that better align with societal values and (2) it develops the foundation to blend deep learning with mathematical optimization to enable the integration of knowledge, constraints, and physical principles into learning models.\nHe is a recipient of the 2022 NSF CAREER award, the 2022 Amazon Research Award, the 2022 Google Research Scholar Award, the 2022 Caspar Bowden PET award, the 2021 ISSNAF Mario Gerla Young Investigator Award, the 2021 ACP Early Career Researcher Award, the 2017 AI*AI Best AI dissertation award, and several best paper awards. He is also actively involved in the organization of several workshops, including the Privacy-Preserving Artificial Intelligence workshop at AAAI, the Algorithmic Fairness through the lens of Causality and Privacy at NeurIPS, and the Optimization and Learning in multiagent systems workshop at AAMAS.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0170805f2a3ba6b472559d2fddca9631","permalink":"https://lix.polytechnique.fr/ethicalai/author/ferdinando-fioretto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/ferdinando-fioretto/","section":"authors","summary":"Title: Privacy and Fairness in Societal Systems Abstract Differential Privacy has become the go-to approach for protecting sensitive information in data releases and learning tasks that are used for critical decision processes.","tags":null,"title":"Ferdinando Fioretto","type":"authors"},{"authors":["giada"],"categories":null,"content":"Title: Conversational AI Ethics: Philosophy and Real-World Case Studies Abstract In the rapidly changing landscape of Machine Learning, Large Language Models (LLMs) are taking a central role in both industry and academia. However, this swift progression brings forth ethical dilemmas and possible societal repercussions. The keynote will explore the philosophical foundations of ethics as they relate to ML and delve into real-world case studies. Specifically, it will highlight open science initiatives like BigScience, which collaboratively developed and deployed a multilingual LLM, BLOOM, and its accompanying dataset, ROOTS.\nShort bio Giada Pistilli is a philosophy researcher specializing in ethics applied to Conversational AI. Her research mainly focuses on ethical frameworks, value theory, applied and descriptive ethics. After obtaining a master’s degree in ethics and political philosophy at Sorbonne University, she pursued her doctoral research in the same faculty. Giada is also the Principal Ethicist at Hugging Face, where she conducts philosophical and interdisciplinary research on AI Ethics and content moderation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7ea271f2ae4737f229f8f302d40bdd90","permalink":"https://lix.polytechnique.fr/ethicalai/author/giada-pistilli/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/giada-pistilli/","section":"authors","summary":"Title: Conversational AI Ethics: Philosophy and Real-World Case Studies Abstract In the rapidly changing landscape of Machine Learning, Large Language Models (LLMs) are taking a central role in both industry and academia.","tags":null,"title":"Giada Pistilli","type":"authors"},{"authors":["ljupcho"],"categories":null,"content":"Title: The Explanations One Needs for the Explanations One Gives: Thoughts on the Epistemic Link between Explainable AI and Causal (Evidentiary) Explanations under the EU’s AI Liability Regulation Abstract The aim of this presentation is to bridge the concept of explainability in connection to AI and (legal) evidence. We will specifically explore the epistemology and interrelationship between explanations relative to the (in)accuracy of AI output and causal explanations pertaining to the link between that output and a harm suffered.\nWith explanatory accuracy as fil rouge, our analytical framework includes two main theoretical touchstones (and corresponding methodologies). The first strand consists of general knowledge construction theory and the epistemology of legal evidence. This strand will inform us of the conditions that need to be met for explanations pertaining to AI output and those pertaining to causality in connection to that output to be considered as accurate (or at least plausible). The second theoretical touchstone is the theory of justice and procedural fairness. The choice of this strand is justified by the fact that trials are the privileged epistemic contexts where accurate explanations on disputed facts are sought. To meet the standards of accuracy required for those, litigants in AI liability disputes should be able to have procedural entitlements that could allow them to give evidence and explain causation in conditions of procedural parity. In light of this (procedural) equality requirement that should frame the pursuit of fact-accuracy in adjudicatory contexts, the key analytical referent for this study is the theory of procedural abilities i.e. entitlements that litigants should enjoy in order to effectively make their views known before a court.\nAgainst this backdrop, and with a focus on the European Union’s (EU) regulation of AI, we will seek to answer two questions: 1. in cases of harm occasioned by the use of an AI system, does the accuracy of causal explanations in AI-related disputes depend on the accuracy of the explanations given on a system’s functionalities? 2. in the affirmative, should the applicable systems of evidence in the EU include the procedural ability for litigants to request and/or give evidence and explanation on how a given system had caused harm? To answer these questions, we will critically examine the systems of evidence in the EU’s upcoming procedural regulation on AI namely, the AI Liability Directive (AILD) and the Revised Product Liability Directive (RPLD). Both instruments include the right to request evidence (and explanation) for victims of AI-related harm, only not for the purpose of uncovering how an AI system actually caused harm (post hoc explainability), but for the purpose of determining whether a human agent (programmer or user) complied with technical standardization legislation, such as the AI Act (ad hoc explainability). Based on an analysis of the available (mostly North American) caselaw on AI liability, the EU’s systems of evidence are open to criticism. First, said caselaw reveals a trend of litigants consistently seeking evidence on how a given system actually caused harm. To this end, they naturally require post hoc explanations. The examined caselaw also reveals that ‘opening the black box’ is not always feasible, pushing courts to request expert evidence that can support arguments on the causal link between an AI system and a harm suffered. By limiting the evidence (and corresponding explanations) to ad hoc epxlainability (i.e. the compliance with the technical standards in the AI Act), the AILD and R-PLD do not seem to leave much room for litigants to request additional evidence - reverse engineering or expertise - that could provide them with the explanations they need to effectively argue causation.\nSecond, neither the AILD nor the R-PLD mention the proof of reliance on (harmful) automated decisions. This is the missing explanatory piece in the instruments considered: as the examined national caselaw shows, the explanation that victims highlight as necessary is - again - not whether a human agent complied with applicable technical standards. They seek explanations on the reasons why that agent believed they should rely on a given decision (the noteworthy point being that those reasons may or may not be rooted in the agent’s compliance with an instrument like the AI Act).\nPerhaps, when the AILD and R-PLD become binding, court practice will interpret their provisions in a way that will enhance the litigants’ procedural abilities to request the evidence they need in order to better explain and debate causation. However, until national and EU courts begin applying these instruments, we remain in the wait-and-see zone and can but speculate on how they ought to apply, so that the basic requirements of fairness (like the equality of arms) can be fully observed.\nShort bio Ljupcho Grozdanovski is a Permanent Research Associate (National …","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f8f16fe36cfde5c58f1e8cf01d26e31d","permalink":"https://lix.polytechnique.fr/ethicalai/author/ljupcho-grozdanovski/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/ljupcho-grozdanovski/","section":"authors","summary":"Title: The Explanations One Needs for the Explanations One Gives: Thoughts on the Epistemic Link between Explainable AI and Causal (Evidentiary) Explanations under the EU’s AI Liability Regulation Abstract The aim of this presentation is to bridge the concept of explainability in connection to AI and (legal) evidence.","tags":null,"title":"Ljupcho Grozdanovski","type":"authors"},{"authors":["mario"],"categories":null,"content":"Short bio Mário S. Alvim is an Assistant Professor of Computer Science at the Federal University of Minas Gerais (UFMG). His research interests include formal methods, particularly quantitative information flow, for security, privacy, fairness, transparency, and other social issues related to responsible computing. He is a guest engineer at Inria Saclay in the COMETE team as part of the ERC ADG HYPATIA project of Catuscia Palamidessi.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"454acc1cf8bead111374b203a9357c0b","permalink":"https://lix.polytechnique.fr/ethicalai/author/mario-s.-alvim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/mario-s.-alvim/","section":"authors","summary":"Short bio Mário S. Alvim is an Assistant Professor of Computer Science at the Federal University of Minas Gerais (UFMG). His research interests include formal methods, particularly quantitative information flow, for security, privacy, fairness, transparency, and other social issues related to responsible computing.","tags":null,"title":"Mário S. Alvim","type":"authors"},{"authors":["natasha"],"categories":null,"content":"Short bio Natasha is a lecturer in Cybersecurity with special interests in privacy-preserving technologies and mathematical techniques for analysing information leaks in secure systems. She holds an undergraduate degree in Pure Mathematics and Computer Science from the University of Sydney, and a PhD in Computing from Macquarie University and École Polytechnique in France. Natasha’s research interests are in the mathematical foundations of data privacy, particularly involving differential privacy, natural language processing or machine learning. Her work involves probabilistic reasoning using quantitative information flow techniques which derive from information-theoretic principles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5485279b4076732af85041e343c94344","permalink":"https://lix.polytechnique.fr/ethicalai/author/natasha-fernandes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/natasha-fernandes/","section":"authors","summary":"Short bio Natasha is a lecturer in Cybersecurity with special interests in privacy-preserving technologies and mathematical techniques for analysing information leaks in secure systems. She holds an undergraduate degree in Pure Mathematics and Computer Science from the University of Sydney, and a PhD in Computing from Macquarie University and École Polytechnique in France.","tags":null,"title":"Natasha Fernandes","type":"authors"},{"authors":["ramon"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"80c3f72b71a1d6d6a3e8cb6b2b0af52a","permalink":"https://lix.polytechnique.fr/ethicalai/author/ramon-goncalves-gonze/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/ramon-goncalves-gonze/","section":"authors","summary":"","tags":null,"title":"Ramon Gonçalves Gonze","type":"authors"},{"authors":["ruta"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3ed14f97fa7d351cfc6637fafe0e3ee2","permalink":"https://lix.polytechnique.fr/ethicalai/author/ruta-binkyte/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/ruta-binkyte/","section":"authors","summary":"","tags":null,"title":"Ruta Binkyte","type":"authors"},{"authors":["szilvia"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ec2c3572e8e5cfda97d70ca58e015ac3","permalink":"https://lix.polytechnique.fr/ethicalai/author/szilvia-lestyan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/author/szilvia-lestyan/","section":"authors","summary":"","tags":null,"title":"Szilvia Lestyan","type":"authors"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://lix.polytechnique.fr/ethicalai/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/ethicalai/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://lix.polytechnique.fr/ethicalai/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/ethicalai/people/","section":"","summary":"","tags":null,"title":"Tour","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5700fd0596aa6e530e386df106098d7e","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/yusuke-kawamoto/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/yusuke-kawamoto/","section":"presentations","summary":"","tags":null,"title":"A Taxonomy of Security Threats, Vulnerabilities, and Controls of AI Systems","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"953b1f0b42cba63284934db387672a74","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/ayoub-ajarra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/ayoub-ajarra/","section":"presentations","summary":"","tags":null,"title":"Active Fourier Verifier: PAC Estimation of Model Properties with Influence Functions and Fourier Representations","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ec7ae8444100d22545b8f79731e89b56","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/martina-cinquini/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/martina-cinquini/","section":"presentations","summary":"","tags":null,"title":"CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7e53d7a367f78c35957a42a937a42ee3","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/michael-perrot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/michael-perrot/","section":"presentations","summary":"","tags":null,"title":"Differential Privacy has Bounded Impact on Fairness in Classification","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e0b7bda7f8109573dfa8258fa2524c8","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/natasha-fernandes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/natasha-fernandes/","section":"presentations","summary":"","tags":null,"title":"Differential privacy surprises: utility is not (always) monotonic on epsilon","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"00978cfaaa984826ce9ca28645be0331","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/laurynas-adomaiti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/laurynas-adomaiti/","section":"presentations","summary":"","tags":null,"title":"Dual Use Concerns of Generative AI and Large Language Models","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1fa54ea7660b5ba76ccb5450f2cd71f2","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/andreas-athanasiou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/andreas-athanasiou/","section":"presentations","summary":"","tags":null,"title":"Enhancing Metric Privacy With a Shuffler","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bb4fbde3aa0b39a88faebd5a6bc31e50","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/alina-leidinger/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/alina-leidinger/","section":"presentations","summary":"","tags":null,"title":"How safe are LLMs compared to search engine autocompletions? A bias check-up","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"652a0140e01693a8091ba15f91b75fe6","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/arturas-grumulaitis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/arturas-grumulaitis/","section":"presentations","summary":"","tags":null,"title":"Legal regulation of AI and morality in the context of natural law and legal positivism -- *Arturas Grumulaitis* \u003cbr/\u003e *Vilnius University*","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"b6b77390bcb245079d4a34893db29fd8","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/jan-aalmoes/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/jan-aalmoes/","section":"presentations","summary":"","tags":null,"title":"Leveraging fairness to quantify attribute inference attacks success","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"854ab70cc3166c5064097eefb5bf47e9","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/miguel-couceiro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/miguel-couceiro/","section":"presentations","summary":"","tags":null,"title":"Mitigating inherent biases in language models by reinforcement learning","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fec98a534c37b6b1bebe13051d7ea530","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/achraf-azize/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/achraf-azize/","section":"presentations","summary":"","tags":null,"title":"On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"84b3df95f5225fbd0c3d9bc848c3ad38","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/karima-makhlouf/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/karima-makhlouf/","section":"presentations","summary":"","tags":null,"title":"On the Impact of Local Differential Privacy on Fairness: A Formal Approach","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5257df59a6f72023ffe594728f51caee","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/konstantin-genin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/konstantin-genin/","section":"presentations","summary":"","tags":null,"title":"Performativity and Prospective Fairness","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bec9ae9ea16a1683fd26a01981d6cf16","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/anais-resseguier/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/anais-resseguier/","section":"presentations","summary":"","tags":null,"title":"The need for an interdisciplinary approach to AI research Developing a research ethics framework for AI","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bc272ad5e0bd73bd902c290ba6b545b4","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/tabouy-laure/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/tabouy-laure/","section":"presentations","summary":"","tags":null,"title":"What ethics does neuroethics bring to bear on the issue of convergence of AI and neurosciences?","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"bd2141573d9371fcb3f0a810cbacb44d","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/ambre-davat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/ambre-davat/","section":"presentations","summary":"","tags":null,"title":"What is the meaning of \"bias\" in AI?","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"fe189e2208233b63052b994b6e767cdc","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/jerome-cooman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/jerome-cooman/","section":"presentations","summary":"","tags":null,"title":"When artificial intelligence goes fishing: On the importance of complete and representative training data of AI-driven competition law enforcement","type":"landing"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0a38ec888254332232ff96e09a13580b","permalink":"https://lix.polytechnique.fr/ethicalai/presentations/thibault-laugel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/ethicalai/presentations/thibault-laugel/","section":"presentations","summary":"","tags":null,"title":"When mitigating bias is unfair: predictive multiplicity in algorithmic fairness","type":"landing"}]